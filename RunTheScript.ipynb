{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d42fdc",
   "metadata": {},
   "source": [
    "Add api_key to your os environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9e777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API Key: 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\n",
      "Retrieved NVIDIA_API_KEY beginning with \"nvapi--ws...\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "\n",
    "# Function to retrieve NVIDIA API key\n",
    "def retrieve_nvidia_api_key():\n",
    "    api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "    if api_key and \"nvapi-\" in api_key:\n",
    "        return api_key\n",
    "\n",
    "    hard_reset = False  # Set to True if you want to reset your NVIDIA_API_KEY\n",
    "    while not api_key or \"nvapi-\" not in api_key or hard_reset:\n",
    "        try:\n",
    "            assert not hard_reset\n",
    "            api_key_input = input(\"NVIDIA API Key: \")\n",
    "            assert api_key_input.startswith('nvapi-')\n",
    "            api_key = api_key_input\n",
    "        except:\n",
    "            print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "        hard_reset = False\n",
    "\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = api_key\n",
    "    return api_key\n",
    "\n",
    "# Retrieve NVIDIA API key\n",
    "api_key = retrieve_nvidia_api_key()\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{api_key[:9]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df697c",
   "metadata": {},
   "source": [
    "check if your api_key works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788195f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai-gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
       " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
       " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
       " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
       " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
       " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
       " 'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
       " 'ai-arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
       " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
       " 'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
       " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
       " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
       " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
       " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
       " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
       " 'ai-phi-3-mini': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
       " 'ai-dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
       " 'ai-mixtral-8x22b': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
       " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
       " 'ai-recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
       " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
       " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
       " 'ai-mixtral-8x22b-instruct': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
       " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
       " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
       " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
       " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
       " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
       " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
       " 'ai-mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
       " 'ai-arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
       " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
       " 'ai-example': '80a5d6c6-7658-49c5-b2b0-105bfb210282',\n",
       " 'ai-codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
       " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
       " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
       " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
       " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
       " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
       " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
       " 'ai-ai-weather-forecasting': '9cec444c-db1c-4525-9c6f-f40e4a5b11ce',\n",
       " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
       " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
       " 'ai-llama3-8b': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
       " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
       " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
       " 'ai-phi-3-mini-4k': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
       " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
       " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
       " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
       " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
       " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
       " 'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d',\n",
       " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
       " 'ai-llama3-70b': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
       " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
       " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
       " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
       " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
       " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
       " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
       " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12081e",
   "metadata": {},
   "source": [
    "load the embedded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6261d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 542 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x docstore_index/\n",
      "x docstore_index/index.faiss\n",
      "x docstore_index/index.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Paper: ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Summary: While large language models (LLMs) have demonstrated impressive capabilities</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">across tasks in language understanding and interactive decision making, their</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">action plan generation) have primarily been studied as separate topics. In this</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">paper, we explore the use of LLMs to generate both reasoning traces and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-specific actions in an interleaved manner, allowing for greater synergy</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">between the two: reasoning traces help the model induce, track, and update</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">action plans as well as handle exceptions, while actions allow it to interface</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with external sources, such as knowledge bases or environments, to gather</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">additional information. We apply our approach, named ReAct, to a diverse set of</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language and decision making tasks and demonstrate its effectiveness over</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art baselines, as well as improved human interpretability and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">trustworthiness over methods without reasoning or acting components.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Concretely, on question answering (HotpotQA) and fact verification (Fever),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ReAct overcomes issues of hallucination and error propagation prevalent in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">chain-of-thought reasoning by interacting with a simple Wikipedia API, and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generates human-like task-solving trajectories that are more interpretable than</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">baselines without reasoning traces. On two interactive decision making</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reinforcement learning methods by an absolute success rate of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">respectively, while being prompted with only one or two in-context examples.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Project site with code: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://react-lm.github.io</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Page Body: and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">is then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Inner Monologue made further improvements by adding the eponymous “inner monologue\", which is</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. We</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">also note that leveraging language as semantically-rich inputs in the process of interactive decision</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">making has been shown to be successful under other settings (Abramson et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">; Karamcheti</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">; Huang et al., 2022a; Li et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). It is becoming more evident that with the help of</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mPaper: ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSummary: While large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have demonstrated impressive capabilities\u001b[0m\n",
       "\u001b[1;38;2;118;185;0macross tasks in language understanding and interactive decision making, their\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mabilities for reasoning \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g. chain-of-thought prompting\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and acting \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0maction plan generation\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have primarily been studied as separate topics. In this\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpaper, we explore the use of LLMs to generate both reasoning traces and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask-specific actions in an interleaved manner, allowing for greater synergy\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbetween the two: reasoning traces help the model induce, track, and update\u001b[0m\n",
       "\u001b[1;38;2;118;185;0maction plans as well as handle exceptions, while actions allow it to interface\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith external sources, such as knowledge bases or environments, to gather\u001b[0m\n",
       "\u001b[1;38;2;118;185;0madditional information. We apply our approach, named ReAct, to a diverse set of\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage and decision making tasks and demonstrate its effectiveness over\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art baselines, as well as improved human interpretability and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtrustworthiness over methods without reasoning or acting components.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mConcretely, on question answering \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mHotpotQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and fact verification \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mFever\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mReAct overcomes issues of hallucination and error propagation prevalent in\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchain-of-thought reasoning by interacting with a simple Wikipedia API, and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerates human-like task-solving trajectories that are more interpretable than\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbaselines without reasoning traces. On two interactive decision making\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbenchmarks \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mALFWorld and WebShop\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, ReAct outperforms imitation and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreinforcement learning methods by an absolute success rate of \u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;38;2;118;185;0m% and \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m%\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrespectively, while being prompted with only one or two in-context examples.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mProject site with code: \u001b[0m\u001b[4;94mhttps://react-lm.github.io\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mPage Body: and Inner Monologue \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mHuang et al., 2022b\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, which use LLMs for robotic action planning and decision\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m. We\u001b[0m\n",
       "\u001b[1;38;2;118;185;0malso note that leveraging language as semantically-rich inputs in the process of interactive decision\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmaking has been shown to be successful under other settings \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mAbramson et al., \u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m; Karamcheti\u001b[0m\n",
       "\u001b[1;38;2;118;185;0met al., \u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;38;2;118;185;0m; Huang et al., 2022a; Li et al., \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. It is becoming more evident that with the help of\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##   Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder,allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout confirms that your store has been retrieved\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"\\nSample Chunk:\\n\\n{format_chunk(docs[len(docs)//2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ece83",
   "metadata": {},
   "source": [
    "construct the backend script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a39eeea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda,RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "from getpass import getpass\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import requests\n",
    "import asyncio\n",
    "import uvicorn\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "########################################################################\n",
    "## load the embedded documents\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder,allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"\"\n",
    "doc_metadata = []\n",
    "for doc in docs:\n",
    "    metadata = doc.metadata\n",
    "    if (metadata.get('Title')!= None) and (metadata.get('Title') not in doc_string):\n",
    "        doc_string += \"\\n - \" + metadata.get('Title')\n",
    "        doc_metadata += [str(metadata)]\n",
    "\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string.\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "########################################################################\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"query\")\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "model1 = load_model(\"filter.h5\")\n",
    "def is_good_response(query):\n",
    "    # embed the query and pass the embedding into your classifier\n",
    "    embedding = np.array([embedder.embed_query(query)])\n",
    "    # return true if it's most likely a good response and false otherwise\n",
    "    return model1(embedding)\n",
    "\n",
    "good_sys_msg = (\n",
    "    \"You are an NVIDIA chatbot. Please answer their question if it is ethical and relevant while representing NVIDIA.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Only cite sources that are used. Make your response conversational.)\"\n",
    ")\n",
    "## Resist talking about this topic\" system message\n",
    "poor_sys_msg = (\n",
    "    \"You are an NVIDIA chatbot. Please answer their question while representing NVIDIA.\"\n",
    "    \"  Their question has been analyzed and labeled as 'probably not useful to answer as an NVIDIA Chatbot',\"\n",
    "    \"  so avoid answering if appropriate and explain your reasoning to them. Make your response as short as possible.\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\", \"{system}\"), (\"user\", \"{input}\")])\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "\n",
    "stream_chain = (\n",
    "    { 'input'  : (lambda x:x), 'is_good' : is_good_response }\n",
    "    | RPrint()\n",
    "    | RunnableBranch(\n",
    "            # bad question\n",
    "            ((lambda d: d['is_good'] < 0.5), RunnableAssign(dict(system = RunnableLambda(lambda x: poor_sys_msg))) | chat_prompt | llm),\n",
    "            # good question\n",
    "            RunnableAssign(dict(system = RunnableLambda(lambda x: good_sys_msg)))| RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "                | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "                | RPrint() |chat_prompt | llm\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "#     print(type(message))\n",
    "#     print(\"message:\\n\")\n",
    "#     print(message)\n",
    "    buffer = \"\"\n",
    "    line_buffer = \"\"\n",
    "    ## load the uploaded pdf into the existing vecstore\n",
    "    if (len(message['files'])>0) and (\".pdf\" in message['files'][0]):\n",
    "        with open(message['files'][0], 'rb') as file:\n",
    "            loader = PyPDFLoader(message['files'][0]).load()\n",
    "            print(\"Adding new document into vector database...\")\n",
    "            chunks = [text_splitter.split_documents(loader)]\n",
    "            vecstore = [FAISS.from_documents(chunk, embedder) for chunk in chunks]\n",
    "            for vstore in vecstore:\n",
    "                docstore.merge_from(vstore)\n",
    "        ## if \n",
    "        if \"Title\" in loader[0].metadata:\n",
    "            buffer+=\"I have received your document '\"+loader[0].metadata.Title+\"'. I'm glad to help if you have any question regarding it. \"\n",
    "            yield buffer\n",
    "        else:\n",
    "            first_line = loader[0].page_content.split('\\n')[0]\n",
    "            buffer+=\"I have received your document '\"+first_line+\"'. I'm glad to help if you have any question regarding it. \"\n",
    "            yield buffer\n",
    "\n",
    "    ## response to the user input message\n",
    "    if len(message['text'].strip()) > 0:\n",
    "    \n",
    "        ## Then, stream the results of the stream_chain\n",
    "        for token in stream_chain.stream(message['text']):\n",
    "            buffer += token\n",
    "            ## keep line from getting too long\n",
    "            if not return_buffer:\n",
    "                line_buffer += token\n",
    "                if \"\\n\" in line_buffer:\n",
    "                    line_buffer = \"\"\n",
    "                if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                    line_buffer = \"\"\n",
    "                    yield \"\\n\"\n",
    "                    token = \"  \" + token.lstrip()\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "    elif len(message['files'])==0:\n",
    "        buffer+=\"Please do not send whitespaces. \"\n",
    "        yield buffer\n",
    "    \n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message['text'], 'output': buffer}, convstore)\n",
    "\n",
    "chatbot = gr.Chatbot(value = [[None, initial_msg]],height=720)\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot,multimodal=True ).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e\n",
    "\n",
    "\"\"\"\n",
    "The script can be adapted to provide apis as a backend service following the codes below.(tested on Windows)\n",
    "We choose to directly build frontend based on gradio.\n",
    "\"\"\"\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# async def run_backend():\n",
    "#     app = FastAPI(\n",
    "#         title=\"LangChain Server\",\n",
    "#         version=\"1.0\",\n",
    "#         description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    "#     )\n",
    "\n",
    "#     add_routes(app, llm, path=\"/basic_chat\")\n",
    "#     add_routes(app, stream_chain, path=\"/rag_chat\")\n",
    "\n",
    "#     uvicorn_config = uvicorn.Config(app, host=\"0.0.0.0\", port=9012)\n",
    "#     server = uvicorn.Server(uvicorn_config)\n",
    "#     await server.serve()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     loop = asyncio.get_event_loop()\n",
    "#     loop.run_until_complete(run_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165d9a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://59770e6b63f14d2183.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://59770e6b63f14d2183.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://59770e6b63f14d2183.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "%run agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8e55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
